{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377e1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy tqdm\n",
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca1901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_mentioned_people(raw_names):\n",
    "    cleaned = []\n",
    "\n",
    "    for name in raw_names:\n",
    "        # Remove common dialogue verbs (e.g., saidBradley)\n",
    "        name = re.sub(r\"^(said|explained|wrote|noted|added)\", \"\", name, flags=re.IGNORECASE)\n",
    "\n",
    "        # Strip whitespace and filter short/empty\n",
    "        name = name.strip()\n",
    "        if len(name) < 3:\n",
    "            continue\n",
    "\n",
    "        # Skip known non-person entities\n",
    "        if name.lower() in [\"quanta magazine\", \"mark belan\", \"samantha mash\"]:  # add more if needed\n",
    "            continue\n",
    "\n",
    "        cleaned.append(name)\n",
    "\n",
    "    # Deduplicate and preserve order\n",
    "    final = list(dict.fromkeys(cleaned))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Load spaCy transformer model (NER)\n",
    "nlp = spacy.load(\"en_core_web_trf\")  # Or use \"en_core_web_sm\" for faster, smaller model\n",
    "\n",
    "# Scrape article content\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, verify=False, timeout=15)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        return \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to scrape {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Run spaCy NER to extract people\n",
    "from collections import Counter\n",
    "\n",
    "def extract_people_with_spacy(article):\n",
    "    doc = nlp(article[\"content\"])\n",
    "    raw_people = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    cleaned_people = [re.sub(r\"^(said|explained|noted|added)\", \"\", p, flags=re.IGNORECASE).strip() for p in raw_people]\n",
    "    filtered_people = [p for p in cleaned_people if len(p) >= 3 and p.lower() not in {\"quanta magazine\", \"mark belan\", \"samantha mash\"}]\n",
    "\n",
    "    # Count occurrences\n",
    "    name_counts = Counter(filtered_people)\n",
    "\n",
    "    return {\n",
    "        \"author\": article[\"author\"],\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": article[\"url\"],\n",
    "        \"mentioned_people\": list(name_counts.keys()),\n",
    "        \"mention_counts\": dict(name_counts)\n",
    "    }\n",
    "\n",
    "# def extract_people_with_spacy(article):\n",
    "#     doc = nlp(article[\"content\"])\n",
    "#     raw_people = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "#     cleaned_people = clean_mentioned_people(raw_people)\n",
    "#     return {\n",
    "#         \"author\": article[\"author\"],\n",
    "#         \"title\": article[\"title\"],\n",
    "#         \"url\": article[\"url\"],\n",
    "#         \"mentioned_people\": cleaned_people\n",
    "#     }\n",
    "\n",
    "# Load article metadata (filtered to top authors)\n",
    "with open(\"quanta_top20_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# You can change this to run only a few for demo:\n",
    "articles_to_process = articles\n",
    "\n",
    "# Run extraction\n",
    "results = []\n",
    "\n",
    "for article in tqdm(articles_to_process, desc=\"Processing with spaCy NER\"):\n",
    "    content = scrape_article_content(article[\"url\"])\n",
    "    if not content.strip():\n",
    "        continue\n",
    "    article[\"content\"] = content\n",
    "    extracted = extract_people_with_spacy(article)\n",
    "    results.append(extracted)\n",
    "    time.sleep(1)  # optional, avoid spamming server\n",
    "\n",
    "# Save output\n",
    "with open(\"quanta_ner_people.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Extraction complete! Results saved to quanta_ner_people.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42019e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\A493892\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load transformer-based spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84be154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
